{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions: ##\n",
    "  \n",
    "1) Place this file at the root of the folder you started jupyter notebook in.  \n",
    "2) Create a \"data/\" folder containing all turnstile data you want to parse.  \n",
    "\n",
    "_Example Structure:_  \n",
    "~~~~\n",
    "/MTA_data_parser.ipynb  \n",
    "/data  \n",
    "/data/turnstile_180922.txt\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import requests\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "# Put filepath here\n",
    "data_path = 'data/'\n",
    "fn = 'turnstile_180922.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all log files given range\n",
    "\n",
    "def gen_dates(start, end, days_between):\n",
    "    '''Use datetime module to generate a list of dates yymmdd format given a start range,\n",
    "    end range, and the num of days in between each date (start range is the base)\n",
    "    :start: array of [int(year), int(mon), int(day)] specifying base start date\n",
    "    :end: array of [int(year), int(mon), int(day)] specifying end date of range\n",
    "    :days_between: how many days between each date (starting with start date), default = 7'''\n",
    "    \n",
    "    start = datetime.datetime(*start)\n",
    "    end = datetime.datetime(*end)\n",
    "    step = datetime.timedelta(days=days_between)\n",
    "    dates = []\n",
    "    \n",
    "    while start < end:\n",
    "        dates.append(start.strftime('%y%m%d'))\n",
    "        start += step\n",
    "    return dates\n",
    "\n",
    "def download_logs(date):\n",
    "    '''Given date input, download log file from MTA url:\n",
    "    :date: a date str yymmdd'''\n",
    "    \n",
    "    print('Downloading', 'turnstile_' + date + '.txt')\n",
    "    url = 'http://web.mta.info/developers/data/nyct/turnstile/turnstile_{}.txt'.format(date)\n",
    "    response = requests.get(url)\n",
    "    open('data/turnstile_' + date + '.txt', 'wb').write(response.content)\n",
    "\n",
    "def parallel_dl(dates, threads = 8):\n",
    "    '''Paralellize downloads to n threads\n",
    "    :threads: number of concurrent download threads (default is 8)'''\n",
    "    \n",
    "    results = ThreadPool(threads).imap_unordered(download_logs, dates)\n",
    "\n",
    "start = [2015,1,3]\n",
    "end = [2018,9,23]\n",
    "days_between = 7\n",
    "dates = gen_dates(start, end, days_between)\n",
    "# parallel_dl(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path + fn)\n",
    "df = df.rename(columns=lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use to do preliminary inspection of data\n",
    "\n",
    "def data_inspect(df, resampled=False):\n",
    "    '''Preliminary inspection of data attributes.\n",
    "    :resampled: If this is set to true, the data has been cleaned and some cols\n",
    "    removed/added. Treat differently'''\n",
    "    \n",
    "    print(\"Summarize NAs:\")\n",
    "    print(df.isna().sum())\n",
    "    \n",
    "    u_station = df['STATION'].unique()\n",
    "    print(\"\\nNum Unique Stations:\", len(u_station))\n",
    "    print(u_station)\n",
    "    \n",
    "    u_desc = df['DESC'].unique()\n",
    "    print(\"\\n Unique Descriptions\")\n",
    "    print(u_desc)\n",
    "    \n",
    "    if resampled:\n",
    "        print(\"\\nUnique Time Vals per station\")\n",
    "        for s in u_station:\n",
    "            u_time = df[df['STATION'] == s]['DATETIME'].unique()\n",
    "            print('Station:', s)\n",
    "            print(u_time, '\\n')\n",
    "\n",
    "    else:\n",
    "        print(\"\\nUnique Time Vals per station\")\n",
    "        for s in u_station:\n",
    "            u_time = df[df['STATION'] == s]['TIME'].unique()\n",
    "            print('Station:', s)\n",
    "            print(u_time, '\\n')\n",
    "\n",
    "# data_inspect(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize data before using diff() to find absolute diff in entries/exits.\n",
    "# Zero out counter rollovers (discard)\n",
    "# Aggregate times to ensure 4 hour interval in-between\n",
    "\n",
    "def clean_data(df, name='stop_data', threshold = 20000):\n",
    "    '''Clean up dataframe by grouping by correct cols, finding change in counts\n",
    "    from entries/exits, zeroing out implausible counts. Add a DATETIME column with\n",
    "    a datetime object for ease of grouping in analysis, and to smooth irregular\n",
    "    data reporting intervals. All of this is stored and pickled in a dict whose key\n",
    "    is the unique station name, and value is a df with cols C/A, STATION, UNIT,\n",
    "    SCP, DATE, TIME.\n",
    "    :df: input raw df data to be cleaned\n",
    "    :name: string for name of pickle dictionary that will be outputted\n",
    "    :threshold: a threshold to throw away any |value| greater than '''\n",
    "    \n",
    "    stop_data = {}\n",
    "    u_station = df['STATION'].unique()\n",
    "    \n",
    "    group_order = ['C/A', 'STATION', 'UNIT', 'SCP', 'DATE', 'TIME']\n",
    "    # Sort by Control Area (station), Unit (Remote unit), SCP (turnstile), Date, Time\n",
    "    print(\"Sorting cols\")\n",
    "    df = df.sort_values(by=group_order)\n",
    "    \n",
    "    # Substract next row by prev row for magnitude of change\n",
    "    print(\"Finding row difference in ENTRIES col\")\n",
    "    df['DIFF_ENTRY'] = df['ENTRIES'].diff()\n",
    "    \n",
    "    # Add datetime col to resample data\n",
    "    print(\"Converting to datetime object\")\n",
    "    df['DATETIME'] = df['DATE'] + ' ' + df['TIME']\n",
    "    df['DATETIME'] = pd.to_datetime(df['DATETIME'])\n",
    "    \n",
    "    # Zero out turnstile errors/rollovers\n",
    "    print(\"Cleaning outliers\")\n",
    "    mask1 = ((df['DIFF_ENTRY'] < 0) & (df['DIFF_ENTRY'] > -1*threshold))\n",
    "    mask2 = ((df['DIFF_ENTRY'] < 0) | (df['DIFF_ENTRY'] > threshold))\n",
    "    df.loc[mask1, 'DIFF_ENTRY'] *= -1\n",
    "    df.loc[mask2, 'DIFF_ENTRY'] = 0\n",
    "        \n",
    "    for s in u_station:\n",
    "        stop_data[s] = df[df['STATION'] == s][['C/A', 'UNIT', 'SCP', 'DIFF_ENTRY', 'DATETIME']]\n",
    "    \n",
    "    with open(data_path + name, 'wb') as f:\n",
    "        print('Pickling to', data_path + name, '...')\n",
    "        pickle.dump(stop_data, f)\n",
    "        print('Done!')\n",
    "\n",
    "# clean_data(df, name='stop_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_stop_data(fn, stops, date_range=None, rule=\"4H\", save=False):\n",
    "    ''' Load and plot entry counts in pickled file containing dictionary with\n",
    "    stop names as keys, and dataframe with columns C/A, UNIT, SCP, DIFF_ENTRY, DATETIME.\n",
    "    :fn: this is the filename within the specified data dir above\n",
    "    :stops: stops is an array containing string names of stops of concern\n",
    "    :date_range: is a list with 2 entries, start date and end date,\n",
    "    [\"yyyy-mm-dd\", \"yyyy-mm-dd\"]. Will be used to limit data range.\n",
    "    :rule: this is the data aggregating option, default by to sum counts by 1 Day\n",
    "    :save: give a string name to your graph to be saved as an svg; default does not save'''\n",
    "    \n",
    "    data = pickle.load(open(data_path+fn, 'rb'))\n",
    "    \n",
    "    # Pre-plot formattings\n",
    "    plot_dims = (17,10)\n",
    "    fig, ax = plt.subplots(figsize=plot_dims)\n",
    "    y_name = 'Turnstile Entries'\n",
    "    \n",
    "    if 'H' in rule:\n",
    "        x_format = '%H:%M'\n",
    "        x_name = 'Hour of Day'\n",
    "    else:\n",
    "        x_format = '%m/%d/%y'\n",
    "        x_name = 'Date'\n",
    "    \n",
    "    # Go through each stop of interest to plot\n",
    "    for stop in stops:\n",
    "        stop_data = data[stop]\n",
    "        stop_data = stop_data[['C/A', 'UNIT', 'SCP', 'DIFF_ENTRY', 'DATETIME']]\n",
    "        \n",
    "        # Slice data to range of concern\n",
    "        if date_range:\n",
    "            start_temp = date_range[0].split('-')\n",
    "            start_temp = [int(x) for x in start_temp]\n",
    "            end_temp = date_range[1].split('-')\n",
    "            end_temp = [int(x) for x in end_temp]\n",
    "            start = datetime.datetime(*start_temp)\n",
    "            end = datetime.datetime(*end_temp)\n",
    "            mask = (stop_data['DATETIME'] >= start) & (stop_data['DATETIME'] < end)\n",
    "            stop_data = stop_data.loc[mask]\n",
    "        \n",
    "        # Resample to ensure regular time intervals (sum over irregular time intervals)\n",
    "        grouped = stop_data.resample(rule, on='DATETIME', base=0).sum()\n",
    "\n",
    "        # Plot\n",
    "        date = grouped.index\n",
    "        amount_entry = grouped['DIFF_ENTRY']\n",
    "        sns.lineplot(x=date, y=amount_entry, label = stop, ax=ax)\n",
    "    \n",
    "    # Set formatting\n",
    "    ax.set_xticklabels(labels=date, rotation=70)\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(x_format))\n",
    "    ax.set_xticks(date)\n",
    "    ax.set_xlabel(x_name)\n",
    "    ax.set_ylabel(y_name)\n",
    "    \n",
    "    # Save options\n",
    "    if save:\n",
    "        print('Saving graph to:', data_path + save + '.svg')\n",
    "        plt.savefig(data_path + save + '.svg', format='svg')\n",
    "    \n",
    "stops = ['TIMES SQ-42 ST','34 ST-HERALD SQ',  '14 ST', '42 ST-BRYANT PK' , '14 ST-UNION SQ']\n",
    "date_range = ['2018-1-08', '2018-9-3']\n",
    "\n",
    "plot_stop_data('final_clean_data', stops, date_range, rule='1W', save='entry_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cumulative_plt(fn, date_range=None, max_stations=None, rule=\"4H\", save=False):\n",
    "    ''' Load and plot cumulative entry counts in pickled file containing dictionary with\n",
    "    stop names as keys, and dataframe with columns C/A, UNIT, SCP, DIFF_ENTRY, DATETIME.\n",
    "    :fn: this is the filename within the specified data dir above\n",
    "    :date_range: is a list with 2 entries, start date and end date,\n",
    "    [\"yyyy-mm-dd\", \"yyyy-mm-dd\"]. Will be used to limit data range.\n",
    "    :rule: this is the data aggregating option, default by to sum counts by 1 Day\n",
    "    :save: give a string name to your graph to be saved as an svg; default does not save'''\n",
    "    \n",
    "    data = pickle.load(open(data_path+fn, 'rb'))\n",
    "    cumulative = []\n",
    "    sum_total = 0\n",
    "    if not max_stations:\n",
    "        max_stations=len(data)\n",
    "\n",
    "    # Pre-plot formattings\n",
    "    plot_dims = (17,10)\n",
    "    fig, ax = plt.subplots(figsize=plot_dims)\n",
    "    y_name = 'Cumulative Entries'\n",
    "    \n",
    "    if 'H' in rule:\n",
    "        x_format = '%H:%M'\n",
    "        x_name = 'Hour of Day'\n",
    "    else:\n",
    "        x_format = '%m/%d/%y'\n",
    "        x_name = 'Date'\n",
    "    \n",
    "    # Go through each stop of interest to plot\n",
    "    for stop in data:\n",
    "        stop_data = data[stop]\n",
    "        stop_data = stop_data[['C/A', 'UNIT', 'SCP', 'DIFF_ENTRY', 'DATETIME']]\n",
    "        \n",
    "        # Slice data to range of concern\n",
    "        if date_range:\n",
    "            start_temp = date_range[0].split('-')\n",
    "            start_temp = [int(x) for x in start_temp]\n",
    "            end_temp = date_range[1].split('-')\n",
    "            end_temp = [int(x) for x in end_temp]\n",
    "            start = datetime.datetime(*start_temp)\n",
    "            end = datetime.datetime(*end_temp)\n",
    "            mask = (stop_data['DATETIME'] >= start) & (stop_data['DATETIME'] < end)\n",
    "            stop_data = stop_data.loc[mask]\n",
    "        \n",
    "        # Resample to ensure regular time intervals (sum over irregular time intervals)\n",
    "        grouped = stop_data.resample(rule, on='DATETIME', base=0).sum()\n",
    "        \n",
    "        # Calculate cumulative over time period, then add to dict\n",
    "        total_entries = grouped['DIFF_ENTRY'].sum()\n",
    "        cumulative.append((stop, total_entries))\n",
    "    \n",
    "    # Turn cumulative list of tuples into sorted, descending df\n",
    "    freq = pd.DataFrame(cumulative, columns=['STATION', 'CUMULATIVE ENTRIES'])\n",
    "    freq.sort_values(by = ['CUMULATIVE ENTRIES'], ascending=False, inplace=True)\n",
    "    freq.reset_index(inplace = True, drop=True)  \n",
    "    freq['CUMSUM'] = freq['CUMULATIVE ENTRIES'].cumsum()\n",
    "    freq['CUMPERC'] = freq['CUMSUM']/freq['CUMULATIVE ENTRIES'].sum()\n",
    "    \n",
    "    x_axis = freq['STATION'][:max_stations]\n",
    "    height = freq['CUMPERC'][:max_stations]\n",
    "    \n",
    "    # Plot\n",
    "    pal = sns.color_palette(\"Blues_d\", n_colors=max_stations)\n",
    "    plt.bar(x_axis, height, color=pal)\n",
    "    plt.axvline(x=4.5, color = 'red', linestyle='dashed')\n",
    "    ax.set_xticklabels(labels=x_axis, rotation=70)\n",
    "\n",
    "    # Save options\n",
    "    if save:\n",
    "        print('Saving graph to:', data_path + save + '.svg')\n",
    "        plt.savefig(data_path + save + '.svg', format='svg')\n",
    "\n",
    "date_range = ['2018-1-08', '2018-9-3']\n",
    "cumulative_plt('final_clean_data', date_range, max_stations=20, rule='1W', save='cumulative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entries_for_census(df, census, date_range=None, rule='1D', save=False):\n",
    "    mta_data = pickle.load(open(data_path+df, 'rb'))\n",
    "    census_data = pd.read_csv(data_path+census, sep='\\t')\n",
    "    stops = list(census_data['station'])\n",
    "    cum_entries = []\n",
    "    above200 = []\n",
    "    below200 = []\n",
    "    \n",
    "    for stop in stops:\n",
    "        stop_data = mta_data[stop]\n",
    "        stop_data = stop_data[['C/A', 'UNIT', 'SCP', 'DIFF_ENTRY', 'DATETIME']]\n",
    "        \n",
    "        # Slice data to range of concern\n",
    "        if date_range:\n",
    "            start_temp = date_range[0].split('-')\n",
    "            start_temp = [int(x) for x in start_temp]\n",
    "            end_temp = date_range[1].split('-')\n",
    "            end_temp = [int(x) for x in end_temp]\n",
    "            start = datetime.datetime(*start_temp)\n",
    "            end = datetime.datetime(*end_temp)\n",
    "            mask = (stop_data['DATETIME'] >= start) & (stop_data['DATETIME'] < end)\n",
    "            stop_data = stop_data.loc[mask]\n",
    "        \n",
    "        # Resample to ensure regular time intervals (sum over irregular time intervals)\n",
    "        grouped = stop_data.resample(rule, on='DATETIME', base=0).sum()\n",
    "        \n",
    "        # Calculate cumulative over time period, then add to dict\n",
    "        total_entries = grouped['DIFF_ENTRY'].sum()\n",
    "        above = int(census_data[census_data['station'] == stop]['Abv200K'])\n",
    "        below = int(census_data[census_data['station'] == stop]['100K_200K'])\n",
    "        cum_entries.append(total_entries)\n",
    "        above200.append(above)\n",
    "        below200.append(below)\n",
    "\n",
    "    census_df = pd.DataFrame({'STATION':stops,\n",
    "                              'CUMULATIVE ENTRY':cum_entries,\n",
    "                              'MID INCOME': [a+b for a,b in list(zip(above200, below200))],\n",
    "                              'TOTAL HOUSE': census_data['Total_households']})\n",
    "    \n",
    "    census_df['MID RATIO'] = census_df['MID INCOME']/census_df['TOTAL HOUSE']\n",
    "    census_df.dropna(inplace=True)\n",
    "    census_df.sort_values(by = ['CUMULATIVE ENTRY'], ascending=False, inplace=True)\n",
    "    census_df.reset_index(inplace = True, drop=True)\n",
    "    \n",
    "    # Plot\n",
    "    plot_dims = (17,10)\n",
    "    fig, ax = plt.subplots(figsize=plot_dims)\n",
    "    sns.scatterplot(x = 'CUMULATIVE ENTRY',\n",
    "                    y = 'MID RATIO', \n",
    "                    data = census_df,\n",
    "                    ax = ax)\n",
    "    \n",
    "    top_stations = ['59 ST', '86 ST', '59 ST COLUMBUS', 'CANAL ST',\\\n",
    "                    '23 ST', 'GRD CNTRL-42 ST', '34 ST-PENN STA', 'CITY HALL']\n",
    "    \n",
    "    # Create labels for top clustered stations\n",
    "    for s in top_stations:\n",
    "        mask = census_df['STATION'] == s\n",
    "        c_entry = float(census_df[mask]['CUMULATIVE ENTRY'])\n",
    "        m_ratio = float(census_df[mask]['MID RATIO'])\n",
    "        ax.text(x=c_entry + 250000, y=m_ratio + .001, s=s)\n",
    "        plt.scatter(c_entry, m_ratio, color='r')\n",
    "        \n",
    "    # Save options\n",
    "    if save:\n",
    "        print('Saving graph to:', data_path + save + '.svg')\n",
    "        plt.savefig(data_path + save + '.svg', format='svg')\n",
    "    \n",
    "    return None\n",
    "\n",
    "date_range = ['2018-1-08', '2018-9-3']\n",
    "census_fp = \"mta_census1.csv\"\n",
    "mta = \"final_clean_data\"\n",
    "\n",
    "entries_for_census(mta, census_fp, date_range, save='census_and_income')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
