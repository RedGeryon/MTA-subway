{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions: ##\n",
    "  \n",
    "1) Place this file at the root of the folder you started jupyter notebook in.  \n",
    "2) Create a \"data/\" folder containing all turnstile data you want to parse.  \n",
    "\n",
    "_Example Structure:_  \n",
    "~~~~\n",
    "/MTA_data_parser.ipynb  \n",
    "/data  \n",
    "/data/turnstile_180922.txt\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import datetime\n",
    "import requests\n",
    "%matplotlib inline\n",
    "\n",
    "# Put filepath here\n",
    "data_path = 'data/'\n",
    "fp = data_path + 'turnstile_180922.txt'\n",
    "df = pd.read_csv(fp)\n",
    "df = df.rename(columns=lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all log files given range\n",
    "\n",
    "start = [2015,1,3]\n",
    "end = [2018, 9, 23]\n",
    "days_between = 7\n",
    "\n",
    "def gen_dates(start, end, days_between):\n",
    "    '''Use datetime module to generate a list of dates yymmdd format given a start range, end range,\n",
    "    and the num of days in between each date (start range is the base)\n",
    "    :start: array of [int(year), int(mon), int(day)] specifying base start date\n",
    "    :end: array of [int(year), int(mon), int(day)] specifying end date of range\n",
    "    :days_between: how many days between each date (starting with start date), default = 7'''\n",
    "    \n",
    "    start = datetime.datetime(*start)\n",
    "    end = datetime.datetime(*end)\n",
    "    step = datetime.timedelta(days=days_between)\n",
    "    dates = []\n",
    "    \n",
    "    while start < end:\n",
    "        dates.append(start.strftime('%y%m%d'))\n",
    "        start += step\n",
    "    return dates\n",
    "\n",
    "def download_logs(date):\n",
    "    '''Given date input, download log file from MTA url:\n",
    "    :date: a date str yymmdd'''\n",
    "    \n",
    "    print('Downloading', 'turnstile_' + date + '.txt')\n",
    "    url = 'http://web.mta.info/developers/data/nyct/turnstile/turnstile_{}.txt'.format(date)\n",
    "    response = requests.get(url)\n",
    "    open('data/turnstile_' + date + '.txt', 'wb').write(response.content)\n",
    "\n",
    "def parallel_dl(dates, threads = 8):\n",
    "    '''Paralellize downloads to n threads\n",
    "    :threads: number of concurrent download threads (default is 8)'''\n",
    "    results = ThreadPool(threads).imap_unordered(download_logs, dates)\n",
    "    \n",
    "dates = gen_dates(start, end, days_between)\n",
    "parallel_dl(dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use to do preliminary inspection of data\n",
    "\n",
    "def data_inspect(df, resampled=False):\n",
    "    ''' Preliminary inspection of data attributes\n",
    "    :resampled: If this is set to true, the data has been cleaned and some cols removed/added. Treat differently'''\n",
    "    \n",
    "    print(\"Summarize NAs:\")\n",
    "    print(df.isna().sum())\n",
    "    \n",
    "    u_station = df['STATION'].unique()\n",
    "    print(\"\\nNum Unique Stations:\", len(u_station))\n",
    "    print(u_station)\n",
    "    \n",
    "    u_desc = df['DESC'].unique()\n",
    "    print(\"\\n Unique Descriptions\")\n",
    "    print(u_desc)\n",
    "    \n",
    "    if resampled:\n",
    "        print(\"\\nUnique Time Vals per station\")\n",
    "        for s in u_station:\n",
    "            u_time = df[df['STATION'] == s]['DATETIME'].unique()\n",
    "            print('Station:', s)\n",
    "            print(u_time, '\\n')\n",
    "\n",
    "    else:\n",
    "        print(\"\\nUnique Time Vals per station\")\n",
    "        for s in u_station:\n",
    "            u_time = df[df['STATION'] == s]['TIME'].unique()\n",
    "            print('Station:', s)\n",
    "            print(u_time, '\\n')\n",
    "\n",
    "data_inspect(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize data before using diff() to find absolute diff in entries/exits.\n",
    "# Zero out counter rollovers (discard)\n",
    "# Aggregate times to ensure 4 hour interval in-between\n",
    "\n",
    "def clean_data(df):\n",
    "    '''Clean up dataframe by grouping by correct cols, finding change in counts from entries/exits,\n",
    "    zeroing out implausible counts. Add a DATETIME column with a datetime object for ease of grouping\n",
    "    in analysis, and to smooth irregular data reporting intervals. All of this is stored and pickled\n",
    "    in a dict whose key is the unique station name, and value is a df with cols C/A, STATION, UNIT,\n",
    "    SCP, DATE, TIME.\n",
    "    :df: input raw df data to be cleaned'''\n",
    "    \n",
    "    stop_data = {}\n",
    "    u_station = df['STATION'].unique()\n",
    "    \n",
    "    group_order = ['C/A', 'STATION', 'UNIT', 'SCP', 'DATE', 'TIME']\n",
    "    # Sort by Control Area (station), Unit (Remote unit), SCP (turnstile), Date, Time\n",
    "    df = df.sort_values(by=group_order)\n",
    "    \n",
    "    # Substract next row by prev row for magnitude of change\n",
    "    df['DIFF_ENTRY'] = df['ENTRIES'].diff()\n",
    "    df['DIFF_EXIT'] = df['EXITS'].diff()\n",
    "    \n",
    "    # Add datetime col to resample data\n",
    "    df['DATETIME'] = df['DATE'] + ' ' + df['TIME']\n",
    "    df['DATETIME'] = pd.to_datetime(df['DATETIME'])\n",
    "    \n",
    "    # Zero out turnstile errors/rollovers\n",
    "    df.loc[df['DIFF_ENTRY'] < 0, 'DIFF_ENTRY'] = 0\n",
    "    df.loc[df['DIFF_ENTRY'] > 4000, 'DIFF_ENTRY'] = 0\n",
    "    df.loc[df['DIFF_EXIT'] > 4000, 'DIFF_EXIT'] = 0\n",
    "    df.loc[df['DIFF_EXIT'] < 0, 'DIFF_EXIT'] = 0\n",
    "    \n",
    "    for s in u_station:\n",
    "        stop_data[s] = df[df['STATION'] == s][['C/A', 'UNIT', 'SCP', 'DIFF_ENTRY', 'DIFF_EXIT', 'DATETIME']]\n",
    "    \n",
    "    pickle.dump(stop_data,open('stop_data', 'wb'))\n",
    "    print('Done')\n",
    "clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_stop_data(fn, rule=\"4H\"):\n",
    "    ''' Load and plot entry counts in pickled file containing dictionary with stop names as keys,\n",
    "    and dataframe with columns C/A, UNIT, SCP, DIFF_ENTRY, DATETIME.\n",
    "    :fn: this is the filename within the specified data dir above\n",
    "    :rule: this is the data aggregating option, default by to sum counts by 1 Day'''\n",
    "    \n",
    "    data = pickle.load(open(data_path+fn, 'rb'))\n",
    "    \n",
    "    for stop in data:\n",
    "        if stop== 'TIMES SQ-42 ST':\n",
    "            stop_data = data[stop]\n",
    "            stop_data = stop_data[['C/A', 'UNIT', 'SCP', 'DIFF_ENTRY', 'DATETIME']]\n",
    "\n",
    "            # resample to ensure everything is by 4H interval (sum irregularities in recording time)\n",
    "            grouped = stop_data.resample(rule, on='DATETIME', base=0).sum()\n",
    "            print(stop)\n",
    "            print(grouped)\n",
    "            amount_entry  = list(grouped['DIFF_ENTRY'])\n",
    "            plt.plot(amount_entry)\n",
    "        \n",
    "plot_stop_data('all_data_parsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_last(df):\n",
    "    # Priyanka's First-Last method\n",
    "    turnstiles_df = df\n",
    "    PREV_ENTRIES=turnstiles_df.groupby(['C/A','UNIT','SCP','STATION','DATE']).ENTRIES.first().reset_index()\n",
    "    LAST=turnstiles_df.groupby(['C/A','UNIT','SCP','STATION','DATE']).ENTRIES.last().reset_index()\n",
    "    PREV_ENTRIES['LAST']=LAST['ENTRIES']\n",
    "    PREV_ENTRIES['DAILY']=PREV_ENTRIES['LAST']-PREV_ENTRIES['ENTRIES']\n",
    "    ST59 = PREV_ENTRIES[PREV_ENTRIES['STATION'] == '59 ST']\n",
    "    print(ST59.groupby(['DATE']).sum())\n",
    "    \n",
    "first_last(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
